# 1.xgboost的优势

**正则化**：gbdt没有实现正则化，xgboost的这一特点可以避免过拟合

**并行性：** （提升是一个有序的过程，如何并行的生成树？同一层上的子树可以并行生成、每个节点的划分发现可以并行http://zhanpengfang.github.io/418home.html）

**高适用性**：允许自定义优化目标h和评价标准

**能自动处理缺失值**

**更智能的树剪枝**

**内置的交叉验证**

**可以在上次训练的模型基础上继续训练**



# 2.参数解释

### 2.1一般参数：控制整体功能

**booster**：在每次迭代中选择的模型 （树模型：gbtree【默认】，线性模型：gblinear）

**silent **：训练过程中是否有信息打印出来（0：有， 1：没有）

**nthread**：线程数，若不设置则自动选用最大线程数

### 2.2提升参数：控制单个提升器（一般都是树模型）

**eta**：相当于gbdt里的学习率，默认是0.3，一般选0.01~0.2

**min_child_weight**：每个孩子所含样本占所有样本的最小权重。用cv确定

**max_depth**：默认是6，通常3~10。用cv确定

**gamma**：只有当分裂造成损失值减少时节点才会分裂，gamma定义了最少要减少多少。这个需要被调节

**max_delta_step**：限制每棵树权重改变的最大步长。如果这个参数的值为0，那就意味着没有约束。如果它被赋予了某个正值，那么它会让这个算法更加保守

**subsample[默认1]**：控制对于每棵树，随机采样的比例 0.5~1

**colsample_bytree[默认1]**：每棵随机采样的列数（特征）的占比0.5~1 

**colsample_bylevel[默认1]**：控制树的每一级的每一次分裂，对列数的采样的占比

**lambda**[默认1]：L2正则化项的权重

**alpha[默认0]**：L1正则化项的权重（可以在高维数据集上使用，可以运行的更快）

**scale_pos_weight[默认1]**：在各类别样本十分不平衡时，把这个参数设定为一个正值，可以使算法更快收敛。

### 2.3学习任务参数

**objective[默认reg:linear]**：定义需要被最小化的损失函数 

​	常用的有：1.binary:logistic 二分类的逻辑回归，返回预测的概率(不是类别) 

​			   2.multi:softmax 使用softmax的多分类器，返回预测的类别(不是概率)  

​			   3.multi:softprob 和multi:softmax参数一样，但是返回的是每个数据属于各个类别的概率

**eval_metric**：用于验证数据的度量指标

​	对于回归问题，默认值是rmse，对于分类问题，默认值是error

​	典型值有： 
​		rmse 均方根误差

​		mae 平均绝对误差

​		logloss 负对数似然函数值

​		error 二分类错误率(阈值为0.5)

​		merror 多分类错误率

​		mlogloss 多分类logloss损失函数

​		auc 曲线下面积

**seed(默认0)**：随机数的种子